---
title: Infrastructure Specifications
description: Detailed hardware, network, and storage specifications for the Virgo-Core Proxmox VE homelab infrastructure
---

# Infrastructure Specifications

This document provides detailed specifications for the Virgo-Core Proxmox VE homelab infrastructure.

## Cluster Overview

### Naming Conventions

- **Cluster Name**: Matrix
- **Node Names**: Military phonetic alphabet
  - Foxtrot
  - Golf
  - Hotel

## Hardware Configuration

Each node in the Matrix cluster has identical hardware:

### System

- **Chassis**: MINISFORUM MS-A2 (mini PC)
- **Motherboard**: Shenzhen Meigao F1WSA
- **Audio**: Realtek ALC245 onboard
- **Wireless**: MediaTek WiFi adapter
- **Graphics**: AMD Radeon Graphics (Granite Ridge integrated)

### Compute

- **CPU**: 1× AMD Ryzen 9 9955HX
  - 16 cores / 32 threads
- **RAM**: 2× 32GB A-DATA DDR5 SODIMMs (model CBDAD5S560032G-BAD)
  - Running at 5600 MT/s in dual-channel configuration
  - Total: 64GB per node

### Storage

Each node has three NVMe drives:

- **1× 1TB Crucial P3** (model CT1000P3PSSD8)
  - Device: `nvme0n1`
  - Purpose: Boot disk
  - Partitioning:

    ```bash
    nvme0n1            931.5G disk
    ├─nvme0n1p1         1007K part
    ├─nvme0n1p2            1G part vfat        /boot/efi
    └─nvme0n1p3        930.5G part LVM2_member
    ```

- **2× 4TB Samsung 990 PRO**
  - Devices: `nvme1n1`, `nvme2n1`
  - Purpose: CEPH cluster storage
  - Each: 3.6T usable capacity
  - OSD Configuration: 2 OSDs per NVMe drive = 4 OSDs per node

### Network Interfaces

Each node has four network interfaces:

1. **2× Intel X710 10GbE SFP+** (dual-port)
   - `enp5s0f0np0`: CEPH Public network (10Gbps DAC - Active)
   - `enp5s0f1np1`: CEPH Private network (10Gbps DAC - Active)

2. **1× Realtek RTL8125 2.5GbE**
   - `enp4s0`: Management network (Active)

3. **1× Intel I226-V Gigabit**
   - `enp3s0`: Not currently used

## Network Architecture

### Bridge Configuration

- **vmbr0**: Management bridge
  - IP Range: 192.168.3.0/24
  - VLAN-aware: Yes
  - VLAN ID: 9 (Corosync)
  - Physical interface: enp4s0

- **vmbr1**: CEPH Public network
  - IP Range: 192.168.5.0/24
  - MTU: 9000 (jumbo frames)
  - Physical interface: enp5s0f0np0

- **vmbr2**: CEPH Private network
  - IP Range: 192.168.7.0/24
  - MTU: 9000 (jumbo frames)
  - Physical interface: enp5s0f1np1

### VLAN Configuration

- **vlan9**: Corosync network
  - IP Range: 192.168.8.0/24
  - Parent: vmbr0
  - Purpose: Cluster communication

### Node IP Assignments

| Node     | Management (vmbr0) | CEPH Public (vmbr1) | CEPH Private (vmbr2) | Corosync (vlan9) |
|----------|-------------------|---------------------|----------------------|------------------|
| Foxtrot  | 192.168.3.5/24    | 192.168.5.5/24      | 192.168.7.5/24       | 192.168.8.5/24   |
| Golf     | 192.168.3.6/24    | 192.168.5.6/24      | 192.168.7.6/24       | 192.168.8.6/24   |
| Hotel    | 192.168.3.7/24    | 192.168.5.7/24      | 192.168.7.7/24       | 192.168.8.7/24   |

### Network Notes

- **Gateway**: 192.168.3.1 (via vmbr0)
- **Jumbo Frames**: MTU 9000 required for CEPH networks (vmbr1, vmbr2)
- **Unifi Controller**: Ensure jumbo frames enabled for high-bandwidth ports

## CEPH Storage Configuration

### Target Configuration

- **Monitors**: 1 per node = 3 total
- **Managers**: 1 per node = 3 total
- **OSDs**: 4 per node = 12 total
  - 2 OSDs per nvme1n1 (4TB Samsung 990 PRO)
  - 2 OSDs per nvme2n1 (4TB Samsung 990 PRO)

### Storage Capacity

- **Raw Capacity**: 24TB (12 OSDs × 2TB per OSD)
- **Usable Capacity**: ~12TB (with CEPH replication factor 2)
- **Performance**: NVMe-backed, 10GbE network, jumbo frames

## Example Node Configuration

### Foxtrot - /etc/network/interfaces

```bash
# network interface settings; autogenerated
# Please do NOT modify this file directly, unless you know what
# you're doing.
#
# If you want to manage parts of the network configuration manually,
# please utilize the 'source' or 'source-directory' directives to do
# so.
# PVE will preserve these directives, but will NOT read its network
# configuration from sourced files, so do not attempt to move any of
# the PVE managed interfaces into external files!

auto lo
iface lo inet loopback

auto enp4s0
iface enp4s0 inet manual
#MGMT-Coro

iface enp3s0 inet manual

auto enp5s0f0np0
iface enp5s0f0np0 inet manual
	mtu 9000
#CEPH Public

auto enp5s0f1np1
iface enp5s0f1np1 inet manual
	mtu 9000
#CEPH Private

auto vmbr0
iface vmbr0 inet static
	address 192.168.3.5/24
	gateway 192.168.3.1
	bridge-ports enp4s0
	bridge-stp off
	bridge-fd 0
	bridge-vlan-aware yes
	bridge-vids 9
#MGMT

iface wlp6s0 inet manual

auto vmbr1
iface vmbr1 inet static
	address 192.168.5.5/24
	bridge-ports enp5s0f0np0
	bridge-stp off
	bridge-fd 0
	mtu 9000
#CEPH Public

auto vmbr2
iface vmbr2 inet static
	address 192.168.7.5/24
	bridge-ports enp5s0f1np1
	bridge-stp off
	bridge-fd 0
	mtu 9000
#CEPH Private

auto vlan9
iface vlan9 inet static
	address 192.168.8.5/24
	vlan-raw-device vmbr0
#Corosync

source /etc/network/interfaces.d/*
```

### Foxtrot - /etc/hosts

```bash
127.0.0.1 localhost.localdomain localhost
192.168.3.5 foxtrot.matrix.spaceships.work foxtrot

# The following lines are desirable for IPv6 capable hosts

::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
```

### Foxtrot - /etc/hostname

```bash
foxtrot
```

## Integration Points

### NetBox + PowerDNS

The infrastructure integrates with:

- **NetBox**: Single source of truth for IPAM and infrastructure documentation
- **PowerDNS**: Authoritative DNS server with API integration
- **NetBox PowerDNS Sync Plugin**: Automatic DNS record generation from NetBox data
- **Diode + Orb Agent**: Automated network discovery

### DNS Naming Convention

Format: `<service>-<number>-<purpose>.<domain>`

Example: `docker-01-nexus.spaceships.work`

## Related Documentation

- [docs/goals.md](goals.md) - Project goals and roadmap
- [docs/ansible-migration-plan.md](ansible-migration-plan.md) - Ansible role development plan
- [docs/netbox-powerdns.md](netbox-powerdns.md) - NetBox and PowerDNS integration details
